{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以太坊区块链性能优化 EthOptimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境的基本信息如下表所示，所需库如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Info | Detail |\n",
    "|----------|----------|\n",
    "| action space    | Discrete(5)      |\n",
    "| Observation shape    | (2, )     |\n",
    "| Observation High    | [300000, 25]     |\n",
    "| Observation Low    | [150000, 5]     |\n",
    "| Import    | gym.make(\"EthOptimize\")     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现有对以太坊区块链不同负载环境下的性能数据进行测试，且具有不同的交易吞吐量（TPS），为了获得较高的交易吞吐量，需要学习如何调整已有的控制参数：区块大小以及出块间隔，来获得更高的tps。在场景测试时，选取的区块大小和出块间隔参数是离散的。\n",
    "定义一个以太坊优化类class EthOptimize作为强化学习的自定义环境，类需要包括如下函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EthOptimize(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.upper_bsize = 300000\n",
    "        self.lower_bsize = 150000\n",
    "        self.upper_btime = 25\n",
    "        self.lower_btime = 5\n",
    "        self.action_space = spaces.Discrete(5) # 0, 1, 2, 3, 4\n",
    "        self.observation_space = spaces.Box(np.array([self.lower_bsize, self.upper_bsize]), np.array([self.lower_btime, self.upper_btime]))\n",
    "        self.state = None\n",
    "        self.counts = 0\n",
    "\n",
    "    # 下一步执行函数，从agent收到下一步的action后执行该action，返回state, reward, done, {info}\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        x, y = self.state\n",
    "        if action == 0:\n",
    "            x=x\n",
    "            y=y\n",
    "        elif action == 1:\n",
    "            x = x + 1500\n",
    "            y = y\n",
    "        elif action == 2:\n",
    "            x = x - 1500\n",
    "            y = y\n",
    "        elif action == 3:\n",
    "            x = x\n",
    "            y = y + 1\n",
    "        elif action == 4:\n",
    "            x = x\n",
    "            y = y - 1\n",
    "        self.state = np.array([x, y])\n",
    "        self.counts += 1\n",
    "\n",
    "        done = None # justify border \n",
    "        done = bool(done)\n",
    "\n",
    "        reward = None\n",
    "        '''\n",
    "        if not done:\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            if ...\n",
    "        '''\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    # 重置函数，每一个episode都要重置环境进行下一个轮次的训练\n",
    "    def reset(self):\n",
    "        # state初始化，回到一个初始状态，为下一个周期准备\n",
    "        self.state = np.array([151500, 7])\n",
    "        return self.state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.upper_bsize = 300000 # 区块大小上限\n",
    "    self.lower_bsize = 150000 # 区块大小下限\n",
    "    self.upper_btime = 25 # 出块间隔上限\n",
    "    self.lower_btime = 5 # 出块间隔下限\n",
    "    self.action_space = spaces.Discrete(5) # 0, 1, 2, 3, 4，分别代表的含义在step中定义\n",
    "    self.observation_space = spaces.Box(np.array([self.lower_bsize, self.upper_bsize]), np.array([self.lower_btime, self.upper_btime]))\n",
    "    self.state = None\n",
    "    self.counts = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有五个离散的操作：增加区块大小，减小区块大小、增加出块间隔，减小出块间隔，DoNothing。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "x, y = self.state # x是区块大小，y是出块时间\n",
    "if action == 0:\n",
    "    x=x\n",
    "    y=y\n",
    "elif action == 1:\n",
    "    x = x + 1500\n",
    "    y = y\n",
    "elif action == 2:\n",
    "    x = x - 1500\n",
    "    y = y\n",
    "elif action == 3:\n",
    "    x = x\n",
    "    y = y + 1\n",
    "elif action == 4:\n",
    "    x = x\n",
    "    y = y - 1\n",
    "self.state = np.array([x, y])\n",
    "self.counts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态是一个二维向量：区块大小（blocksize），出块间隔（blocktime）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（需要讨论有几种情况，哪种情况加奖励，哪种减少奖励，例如tps增加获得10点，减少tps扣50点，区块大小和区块间隔的值超过上下界扣100等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = None\n",
    "'''\n",
    "if not done:\n",
    "    reward -= 0.1\n",
    "else:\n",
    "    if ...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对应控制参数的区间内取随机的一个状态作为开始状态，这个状态在reset函数中声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self):\n",
    "    # state初始化，回到一个初始状态，为下一个周期准备\n",
    "    self.state = np.array([151500, 7])\n",
    "    return self.state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讨论当什么时候episode结束，初步给出如下限制：\n",
    "1. 当区块大小超过上下界时\n",
    "2. 当区块间隔超过上下界时\n",
    "3. episode的上限（即当episode达到多少时不再训练  \n",
    "\n",
    "在step中给出是否结束该次训练的结果，用done（一个bool值）表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = None # need to justify border here\n",
    "done = bool(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体参考案例如下：\n",
    "1. [自定义一个强化学习环境](https://zhuanlan.zhihu.com/p/33553076)\n",
    "2. [OpenAI Gym库文档经典案例](https://www.gymlibrary.dev/environments/classic_control/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddqn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
