{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以太坊区块链性能优化 EthOptimize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "环境的基本信息如下表所示，所需库如下所示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Info | Detail |\n",
    "|----------|----------|\n",
    "| action space    | Discrete(5)      |\n",
    "| Observation shape    | (2, )     |\n",
    "| Observation High    | [300000, 25]     |\n",
    "| Observation Low    | [150000, 5]     |\n",
    "| Import    | gym.make(\"EthOptimize\")     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现有对以太坊区块链不同负载环境下的性能数据进行测试，且具有不同的交易吞吐量（TPS），为了获得较高的交易吞吐量，需要学习如何调整已有的控制参数：区块大小以及出块间隔，来获得更高的tps。在场景测试时，选取的区块大小和出块间隔参数是离散的。\n",
    "定义一个以太坊优化类class EthOptimize作为强化学习的自定义环境，类需要包括如下函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EthOptimize(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.upper_bsize = 300000\n",
    "        self.lower_bsize = 150000\n",
    "        self.upper_btime = 25\n",
    "        self.lower_btime = 5\n",
    "        self.action_space = spaces.Discrete(5) # 0, 1, 2, 3, 4\n",
    "        self.observation_space = spaces.Box(np.array([self.lower_bsize, self.upper_bsize]), np.array([self.lower_btime, self.upper_btime]))\n",
    "        self.state = None\n",
    "        self.counts = 0\n",
    "\n",
    "    # 下一步执行函数，从agent收到下一步的action后执行该action，返回state, reward, done, {info}\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        x, y = self.state\n",
    "        if action == 0:\n",
    "            x=x\n",
    "            y=y\n",
    "        elif action == 1:\n",
    "            x = x + 1500\n",
    "            y = y\n",
    "        elif action == 2:\n",
    "            x = x - 1500\n",
    "            y = y\n",
    "        elif action == 3:\n",
    "            x = x\n",
    "            y = y + 1\n",
    "        elif action == 4:\n",
    "            x = x\n",
    "            y = y - 1\n",
    "        self.state = np.array([x, y])\n",
    "        self.counts += 1\n",
    "\n",
    "        done = None # justify border \n",
    "        done = bool(done)\n",
    "\n",
    "        reward = None\n",
    "        '''\n",
    "        if not done:\n",
    "            reward -= 0.1\n",
    "        else:\n",
    "            if ...\n",
    "        '''\n",
    "        \n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    # 重置函数，每一个episode都要重置环境进行下一个轮次的训练\n",
    "    def reset(self):\n",
    "        # state初始化，回到一个初始状态，为下一个周期准备\n",
    "        self.state = np.array([151500, 7])\n",
    "        return self.state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    self.upper_bsize = 300000 # 区块大小上限\n",
    "    self.lower_bsize = 150000 # 区块大小下限\n",
    "    self.upper_btime = 25 # 出块间隔上限\n",
    "    self.lower_btime = 5 # 出块间隔下限\n",
    "    self.action_space = spaces.Discrete(5) # 0, 1, 2, 3, 4，分别代表的含义在step中定义\n",
    "    self.observation_space = spaces.Box(np.array([self.lower_bsize, self.upper_bsize]), np.array([self.lower_btime, self.upper_btime]))\n",
    "    self.state = None\n",
    "    self.counts = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有五个离散的操作：增加区块大小，减小区块大小、增加出块间隔，减小出块间隔，DoNothing。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "x, y = self.state # x是区块大小，y是出块时间\n",
    "if action == 0:\n",
    "    x=x\n",
    "    y=y\n",
    "elif action == 1:\n",
    "    x = x + 1500\n",
    "    y = y\n",
    "elif action == 2:\n",
    "    x = x - 1500\n",
    "    y = y\n",
    "elif action == 3:\n",
    "    x = x\n",
    "    y = y + 1\n",
    "elif action == 4:\n",
    "    x = x\n",
    "    y = y - 1\n",
    "self.state = np.array([x, y])\n",
    "self.counts += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "状态是一个二维向量：区块大小（blocksize），出块间隔（blocktime）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（需要讨论有几种情况，哪种情况加奖励，哪种减少奖励，例如tps增加获得10点，减少tps扣50点，区块大小和区块间隔的值超过上下界扣100等）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward = None\n",
    "'''\n",
    "if not done:\n",
    "    reward -= 0.1\n",
    "else:\n",
    "    if ...\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在对应控制参数的区间内取随机的一个状态作为开始状态，这个状态在reset函数中声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(self):\n",
    "    # state初始化，回到一个初始状态，为下一个周期准备\n",
    "    self.state = np.array([151500, 7])\n",
    "    return self.state "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讨论当什么时候episode结束，初步给出如下限制：\n",
    "1. 当区块大小超过上下界时\n",
    "2. 当区块间隔超过上下界时\n",
    "3. episode的上限（即当episode达到多少时不再训练  \n",
    "\n",
    "在step中给出是否结束该次训练的结果，用done（一个bool值）表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = None # need to justify border here\n",
    "done = bool(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体参考案例如下：\n",
    "1. [自定义一个强化学习环境](https://zhuanlan.zhihu.com/p/33553076)\n",
    "2. [OpenAI Gym库文档经典案例](https://www.gymlibrary.dev/environments/classic_control/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充： 环境与gym库的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当构建完一个简单的模拟器后，通过使用openai的stable-baseline3的check_env来检查构建的模拟器是否存在问题，从而在训练时加载编译环境不会出错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from stable_baselines3.common.env_checker import check_env \n",
    "    import eth_optimize as src\n",
    "    # 如果你安装了pytorch，则使用上面的，如果你安装了tensorflow，则使用from stable_baselines.common.env_checker import check_env\n",
    "    env = src.EthOptimize()\n",
    "    check_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用一个简单的sim实例跑一遍，总结出以下的一些问题，实例文件为“/Users/apple/Documents/GitHub/Double-DQN/Mysim.py”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: 版本匹配问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stable_baselines3后续使用了优化的gym库，即gymnasium，所以在定义时搞清楚引用的版本是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "    check_env(env)\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 409, in check_env\n",
    "    assert isinstance(\n",
    "AssertionError: Your environment must inherit from the gymnasium.Env class cf. https://gymnasium.farama.org/api/env/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: 状态空间的生成\n",
    "\n",
    "引入high和low来避免识别问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/apple/Documents/GitHub/Double-DQN/eth_optimize.py\", line 68, in <module>\n",
    "    env = EthOptimize()\n",
    "  File \"/Users/apple/Documents/GitHub/Double-DQN/eth_optimize.py\", line 13, in __init__\n",
    "    self.observation_space = spaces.Box(np.array([self.lower_bsize, self.upper_bsize]), np.array([self.lower_btime, self.upper_btime]))\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/gymnasium/spaces/box.py\", line 126, in __init__\n",
    "    raise ValueError(\n",
    "ValueError: Some low values are greater than high, low=[150000. 300000.], high=[ 5. 25.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: reset函数\n",
    "\n",
    "新版本的reset函数需接受一个seed作为输入，并且在输出时需要加一个info组成元组输出\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 421, in check_env\n",
    "    env.reset(seed=0)\n",
    "TypeError: reset() got an unexpected keyword argument 'seed'\n",
    "\n",
    "The above exception was the direct cause of the following exception:\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/apple/Documents/GitHub/Double-DQN/eth_optimize.py\", line 71, in <module>\n",
    "    check_env(env)\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 423, in check_env\n",
    "    raise TypeError(\"The reset() method must accept a `seed` parameter\") from e\n",
    "TypeError: The reset() method must accept a `seed` parameter\n",
    "\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/apple/Documents/GitHub/Double-DQN/eth_optimize.py\", line 68, in <module>\n",
    "    check_env(env)\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 461, in check_env\n",
    "    _check_returned_values(env, observation_space, action_space)\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 264, in _check_returned_values\n",
    "    assert isinstance(reset_returns, tuple), \"`reset()` must return a tuple (obs, info)\"\n",
    "AssertionError: `reset()` must return a tuple (obs, info)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: step函数需输出声明是否截断truncate\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"/Users/apple/Documents/GitHub/Double-DQN/Mysim.py\", line 35, in <module>\n",
    "    check_env(env)\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 461, in check_env\n",
    "    _check_returned_values(env, observation_space, action_space)\n",
    "  File \"/Users/apple/opt/anaconda3/envs/ddqn-env/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py\", line 294, in _check_returned_values\n",
    "    assert len(data) == 5, (\n",
    "AssertionError: The `step()` method must return five values: obs, reward, terminated, truncated, info. Actual: 4 values returned.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后验证没有其他错误，可以与gym库互联编译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(ddqn-env)  apple@appledeMacBook-Pro-2 ~/Documents/GitHub/Double-DQN > python Mysim.py\n",
    "(ddqn-env)  apple@appledeMacBook-Pro-2 ~/Documents/GitHub/Double-DQN > \n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddqn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
